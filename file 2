## Linear Discriminant Analysis Steps

1. Compute mean vector of each class
2. Compute scatter matrices for in between and within classes
3. Solve matrix[Sw^-1.SB] to get eigen values and eigen vectors
4. Sort the eigen values in decreasing order and choose k eigen vectors with the highest eigen values. Create W by taking those eigen vectors as columns

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

## Accessing the data

wine=pd.read_csv('https://gist.githubusercontent.com/tijptjik/9408623/raw/b237fa5848349a14a14e5d4107dc7897c21951f5/wine.csv')

wine

# Separation inot features and target

X=wine.drop(['Wine'],axis=1)

X

X.shape

y=wine['Wine']
y

y.shape

## Standardisation of features

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_scaled=sc.fit_transform(X)
X_scaled

## Building the LDA model

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

y.unique()

# n_components <= min(n_classes-1, n_features)
# n_components <= min(3-1, 13)
#               = 2, the dim of the hyperplane onto which we want to project

lda=LinearDiscriminantAnalysis(n_components=2)

## Transformation of the data

X_lda=lda.fit_transform(X_scaled,y)

X_lda

## Visualisation of the transformed data

plt.scatter(X_lda[:,0],X_lda[:,1],c=y);

## Locally Linear Embedding

from sklearn.datasets import make_swiss_roll

X,y=make_swiss_roll(n_samples=1000, random_state=100)
X

y

plt.scatter(X[:,0],X[:,1],c=y)

## Building the model

from sklearn.manifold import LocallyLinearEmbedding

lle=LocallyLinearEmbedding(n_neighbors=10,n_components=2)

## Transforming the data

X_lle=lle.fit_transform(X)
X_lle

## Visualisation

plt.scatter(X_lle[:,0],X_lle[:,1],c=y);

# Exercise
Try LLE with Wine Dataset

X_scaled,y=make_swiss_roll(n_samples=1000, random_state=100)
X_scaled

y

